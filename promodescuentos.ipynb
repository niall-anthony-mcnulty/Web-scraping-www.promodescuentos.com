{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "import urllib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #url based off page number\n",
    "# pageno = 1\n",
    "\n",
    "# # list of urls\n",
    "# arr_url  = []\n",
    "\n",
    "# # loop through pages until pages run out\n",
    "# while pageno < 2000:\n",
    "    \n",
    "    \n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#     }\n",
    "\n",
    "   \n",
    "\n",
    "#     baseurl = \"https://www.promodescuentos.com/nuevas?page=\" + str(pageno)\n",
    "\n",
    "#     # request page and soupify\n",
    "#     r = requests.get(baseurl, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "    \n",
    "#     # error handling for login required links\n",
    "#     try:\n",
    "        \n",
    "#         products = soup.find_all('a', {\"class\": \"cept-thread-image-link\"},href=True)\n",
    "    \n",
    "#         #loop through 'a' elements and extract url\n",
    "#         for links in products:\n",
    "#             arr_url.append(links['href'])\n",
    "\n",
    "        \n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "#     # increment pageno by 1\n",
    "#     pageno += 1\n",
    "\n",
    "\n",
    "# pprint.pprint(arr_url)\n",
    "# print(len(arr_url))\n",
    "# print(pageno)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store urls\n",
    "# df = pd.DataFrame(arr_url, columns = ['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to csv - Save to your own directory\n",
    "\n",
    "# df.to_csv(\"/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/nuevas_urls.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read in URL csv - Load in from your own directory\n",
    "\n",
    "# df_url = pd.read_csv(\"/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/nuevas_urls.csv\", index_col=False)\n",
    "# df_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # drop index col\n",
    "\n",
    "# print(len(df_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list of URLS to iterate over\n",
    "# arr_url = [ x for x in df_url['urls']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW IS EACH INDIVIDUAL DATA POINTS CODE - USED FOR DEBUGGING and UPGRADING if ADDITIONAL CONDITIONALS ARE NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through URLS and obtain degree data\n",
    "\n",
    "\n",
    "# each_url_degrees = []\n",
    "\n",
    "# for urls in arr_url[39:40]:\n",
    "\n",
    "#     #bypass HTTP security & login ( this can't be used by other users - EXPLATION followed-----)\n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#         }\n",
    "    \n",
    "    \n",
    "#     # request page and soupify\n",
    "#     r = requests.get(urls, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "               \n",
    "#     try:\n",
    "#         degrees = soup.find('div',{'class' : \"cept-vote-box\"}).text\n",
    "#         degrees = re.sub('[^-?0-9]', '', degrees)\n",
    "#         # return only digits\n",
    "#         each_url_degrees.append(int(degrees))\n",
    "#     except:\n",
    "#         each_url_degrees.append(None)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# pprint.pprint(each_url_degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through URLS and obtain product data\n",
    "\n",
    "\n",
    "# each_url_product = []\n",
    "\n",
    "# for urls in arr_url[39:40]:\n",
    "\n",
    "#     # Bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----\n",
    "#     # Login into promodescuentos.com and afterwards, follow the instructions in this website https://curlconverter.com/ \n",
    "#     # Copy and paste header parameters below \n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#         }\n",
    "    \n",
    "    \n",
    "#     # request page and soupify\n",
    "#     r = requests.get(urls, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "               \n",
    "#     try:\n",
    "#         product = soup.find('span',{'class' : \"thread-title--item\"}).text\n",
    "#         each_url_product.append(product)\n",
    "#     except:\n",
    "#          each_url_product.append(None)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# pprint.pprint(each_url_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # loop through URLS and obtain final price\n",
    "\n",
    "\n",
    "# each_url_final_price = []\n",
    "\n",
    "# for urls in arr_url[39:40]:\n",
    "\n",
    "#     #bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----)\n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#         }\n",
    "    \n",
    "    \n",
    "#     # request page and soupify\n",
    "#     r = requests.get(urls, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "               \n",
    "#     try:\n",
    "#         final_price = soup.find('span',{'class' : \"cept-tp\"}).text\n",
    "#         substring = 'GRATIS'\n",
    "#         # check if final price is GRATIS - return 0 if so\n",
    "#         if substring in final_price:\n",
    "#             each_url_final_price.append(0)\n",
    "#         else:\n",
    "#             # return only digits (take out special characters to ensure int datatype)\n",
    "#             final_price = re.sub('[^0-9]', '', final_price)\n",
    "#             each_url_final_price.append(int(final_price))\n",
    "#     except:\n",
    "#         each_url_final_price.append(None)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# pprint.pprint(each_url_final_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through URLS and obtain original price\n",
    "\n",
    "\n",
    "# each_url_original_price = []\n",
    "\n",
    "# for urls in arr_url[39:40]:\n",
    "\n",
    "#     #bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----)\n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#         }\n",
    "    \n",
    "    \n",
    "#     # request page and soupify\n",
    "#     r = requests.get(urls, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "               \n",
    "#     try:\n",
    "#         original_price = soup.find('span',{'class' : \"cept-next-best-price\"}).text\n",
    "#         original_price = re.sub('[^0-9]', '', original_price)\n",
    "#         each_url_original_price.append(int(original_price))\n",
    "#     except:\n",
    "#         each_url_original_price.append(None)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# pprint.pprint(each_url_original_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    urls\n",
      "0      https://www.promodescuentos.com/ofertas/league...\n",
      "1      https://www.promodescuentos.com/ofertas/chedra...\n",
      "2      https://www.promodescuentos.com/ofertas/amazon...\n",
      "3      https://www.promodescuentos.com/ofertas/micros...\n",
      "4      https://www.promodescuentos.com/ofertas/cinepo...\n",
      "...                                                  ...\n",
      "39975  https://www.promodescuentos.com/ofertas/adidas...\n",
      "39976  https://www.promodescuentos.com/ofertas/set-pl...\n",
      "39977  https://www.promodescuentos.com/ofertas/amazon...\n",
      "39978  https://www.promodescuentos.com/ofertas/funko-...\n",
      "39979  https://www.promodescuentos.com/ofertas/linio-...\n",
      "\n",
      "[39980 rows x 1 columns]\n",
      "https://www.promodescuentos.com/ofertas/league-of-legends-capsula-prime-gaming-663236\n",
      "https://www.promodescuentos.com/ofertas/chedraui-alfredo-del-mazo-assassins-creed-iii-663234\n",
      "https://www.promodescuentos.com/ofertas/amazon-best-trading-100-cubrebocas-tricapa-663233\n",
      "[False, False, True]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# loop through URLS and obtain free shipping\n",
    "df = pd.read_csv('/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/nuevas_urls.csv')\n",
    "print(df)\n",
    "\n",
    "each_url_free_shipping = []\n",
    "\n",
    "for urls in df['urls'][0:3]:\n",
    "    print(urls)\n",
    "\n",
    "# each_url_free_shipping = []\n",
    "\n",
    "# for urls in arr_url[100:200]:\n",
    "\n",
    "    #bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----)\n",
    "\n",
    "    headers = {\n",
    "        'authority': 'www.promodescuentos.com',\n",
    "        'cache-control': 'max-age=0',\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'none',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "        'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # request page and soupify\n",
    "    r = requests.get(urls, headers=headers).text\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "               \n",
    "    try1 = soup.find('span',{'class' : \"cept-shipping-price\"})\n",
    "    try2 = soup.find('span',{'class' : \"cept-tp\"})\n",
    "    try3 = soup.find('div',{ 'class' : 'threadItem-title'})\n",
    "    try4 = soup.find('span',{'class' : 'overflow--fade'})\n",
    "    try5 = soup.find('span',{'class' : 'text--color-greyShade'})\n",
    "    \n",
    "    # check for envio gratis and return true\n",
    "    substring1 = 'Envío gratis'\n",
    "    try:\n",
    "        \n",
    "        if (try1 is not None) and (substring1 in try1.text):\n",
    "        # append to list\n",
    "            each_url_free_shipping.append(True)\n",
    "        elif (try2 is not None) and (substring1 in try2.text):\n",
    "            each_url_free_shipping.append(True) \n",
    "        elif (try3 is not None) and (substring1 in try3.text):\n",
    "            each_url_free_shipping.append(True)\n",
    "        elif (try4 is not None) and (substring1 in try4.text):\n",
    "            each_url_free_shipping.append(True)\n",
    "        elif (try5 is not None) and (substring1 in try5.text):\n",
    "            each_url_free_shipping.append(True)\n",
    "        else:\n",
    "            each_url_free_shipping.append(False)\n",
    "\n",
    "    except:\n",
    "        each_url_free_shipping.append(False)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "pprint.pprint(each_url_free_shipping)\n",
    "pprint.pprint(len(each_url_free_shipping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through URLS and obtain usernames\n",
    "\n",
    "\n",
    "# each_url_username = []\n",
    "\n",
    "# for urls in arr_url[100:200]:\n",
    "\n",
    "#     #bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----)\n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#         }\n",
    "    \n",
    "    \n",
    "#     # request page and soupify\n",
    "#     r = requests.get(urls, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "               \n",
    "#     try:\n",
    "#         username = soup.find('span',{'class' : \"thread-username\"}).text\n",
    "#         username = re.sub('[\\s*$]', '', username)\n",
    "#         each_url_username.append(username)\n",
    "#     except:\n",
    "#         each_url_username.append(None)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# pprint.pprint(each_url_username)\n",
    "# pprint.pprint(len(each_url_username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    urls\n",
      "0      https://www.promodescuentos.com/ofertas/league...\n",
      "1      https://www.promodescuentos.com/ofertas/chedra...\n",
      "2      https://www.promodescuentos.com/ofertas/amazon...\n",
      "3      https://www.promodescuentos.com/ofertas/micros...\n",
      "4      https://www.promodescuentos.com/ofertas/cinepo...\n",
      "...                                                  ...\n",
      "39975  https://www.promodescuentos.com/ofertas/adidas...\n",
      "39976  https://www.promodescuentos.com/ofertas/set-pl...\n",
      "39977  https://www.promodescuentos.com/ofertas/amazon...\n",
      "39978  https://www.promodescuentos.com/ofertas/funko-...\n",
      "39979  https://www.promodescuentos.com/ofertas/linio-...\n",
      "\n",
      "[39980 rows x 1 columns]\n",
      "https://www.promodescuentos.com/ofertas/league-of-legends-capsula-prime-gaming-663236\n",
      "1\n",
      "[<span class=\"flex--toW3 overflow--wrap-off text--color-greyShade\"><span class=\"lbox--v-3 threadItem-ribbonIcoCell\"><svg class=\"icon icon--clock text--color-greyShade\" height=\"22px\" width=\"22px\"><use xlink:href=\"/assets/img/ico_a1fd8.svg#clock\"></use></svg></span><span class=\"space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2\">oct 27º. 2021 (hace 19 h, 31 m)</span></span>]\n",
      "https://www.promodescuentos.com/ofertas/chedraui-alfredo-del-mazo-assassins-creed-iii-663234\n",
      "2\n",
      "[<span class=\"flex--toW3 overflow--wrap-off text--color-greyShade\"><span class=\"lbox--v-3 threadItem-ribbonIcoCell\"><svg class=\"icon icon--clock text--color-greyShade\" height=\"22px\" width=\"22px\"><use xlink:href=\"/assets/img/ico_a1fd8.svg#clock\"></use></svg></span><span class=\"space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2\">oct 27º. 2021 (hace 19 h, 32 m)</span></span>, <span class=\"flex--toW3 overflow--wrap-off text--color-greyShade\"><span class=\"lbox--v-3 threadItem-ribbonIcoCell\"><svg class=\"icon icon--location text--color-greyShade\" height=\"22px\" width=\"16px\"><use xlink:href=\"/assets/img/ico_a1fd8.svg#location\"></use></svg></span><span class=\"space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2\">Local</span></span>]\n",
      "https://www.promodescuentos.com/ofertas/amazon-best-trading-100-cubrebocas-tricapa-663233\n",
      "1\n",
      "[<span class=\"flex--toW3 overflow--wrap-off text--color-greyShade\"><span class=\"lbox--v-3 threadItem-ribbonIcoCell\"><svg class=\"icon icon--clock text--color-greyShade\" height=\"22px\" width=\"22px\"><use xlink:href=\"/assets/img/ico_a1fd8.svg#clock\"></use></svg></span><span class=\"space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2\">oct 27º. 2021 (hace 19 h, 35 m)</span></span>]\n",
      "['oct 27º. 2021', 'oct 27º. 2021', 'oct 27º. 2021']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# loop through URLS and obtain date \n",
    "\n",
    "df = pd.read_csv('/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/nuevas_urls.csv')\n",
    "print(df)\n",
    "\n",
    "each_url_date = []\n",
    "\n",
    "\n",
    "for urls in df['urls'][0:3]:\n",
    "    print(urls)\n",
    "\n",
    "    #bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----)\n",
    "\n",
    "    headers = {\n",
    "        'authority': 'www.promodescuentos.com',\n",
    "        'cache-control': 'max-age=0',\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'none',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "        'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # request page and soupify\n",
    "    r = requests.get(urls, headers=headers).text\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "               \n",
    "    try:\n",
    "        \n",
    "        # check for post date \n",
    "        dates = soup.find_all('span',{'class' : \"flex--toW3 overflow--wrap-off text--color-greyShade\"})\n",
    "        print(len(dates))\n",
    "        print(dates)\n",
    "\n",
    "        if len(dates) == 0:\n",
    "            # check for publication\n",
    "            pubs = soup.find('span', {'class' :'flex--toW3 overflow--wrap-off text--color-greyShade text--b'})\n",
    "            # if no publication date add None to the dates list/array\n",
    "            if len(pubs) == 0:\n",
    "                each_url_date.append(None)\n",
    "                # if there is a publication date\n",
    "            else:\n",
    "                # return the date inside the brackets using regex and add to the dates list/array\n",
    "                pub_date = pubs.find('span', {'class' :'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "                regex = re.findall('\\((.*)\\)', pub_date)\n",
    "                pub_date = regex[0]\n",
    "                pub_date = re.sub('(Publicado)', '', pub_date)\n",
    "                \n",
    "                # check for string year\n",
    "                substring_year = '20'\n",
    "                # current year\n",
    "                current_year = str(datetime.datetime.now().year)\n",
    "                # if it does have the year, don't append \n",
    "                if substring_year in pub_date:\n",
    "                    pub_date = re.sub('^[ \\t]+', '', pub_date)\n",
    "                    each_url_date.append(pub_date)\n",
    "                else:\n",
    "                    # if it does, concatenate year\n",
    "                    pub_date = re.sub('^[ \\t]+', '', pub_date) + ' ' +  current_year\n",
    "                    each_url_date.append(pub_date)\n",
    "                    \n",
    "\n",
    "        elif soup.find('span', {'class' :'flex--toW3 overflow--wrap-off text--color-greyShade text--b'}):\n",
    "            pubs = soup.find('span', {'class' :'flex--toW3 overflow--wrap-off text--color-greyShade text--b'})\n",
    "            \n",
    "            # if no publication date add None to the dates list/array\n",
    "            if len(pubs) == 0:\n",
    "                each_url_date.append(None)\n",
    "                # if there is a publication date\n",
    "            else:\n",
    "                # return the date inside the brackets using regex and add to the dates list/array\n",
    "                pub_date = pubs.find('span', {'class' :'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "                regex = re.findall('\\((.*)\\)', pub_date)\n",
    "                pub_date = regex[0]\n",
    "                pub_date = re.sub('(Publicado)', '', pub_date)\n",
    "                \n",
    "                # check for string year\n",
    "                substring_year = '20'\n",
    "                # current year\n",
    "                current_year = str(datetime.datetime.now().year)\n",
    "                # if it does have the year, don't append \n",
    "                if substring_year in pub_date:\n",
    "                    pub_date = re.sub('^[ \\t]+', '', pub_date)\n",
    "                    each_url_date.append(pub_date)\n",
    "                else:\n",
    "                    # if it does, concatenate year\n",
    "                    pub_date = re.sub('^[ \\t]+', '', pub_date) + ' ' + current_year\n",
    "                    each_url_date.append(pub_date)\n",
    "                \n",
    "            \n",
    "\n",
    "        else:\n",
    "            # loop through the classes\n",
    "            for date in dates:\n",
    "            # check for clock icon\n",
    "                if date.find('svg',{'class':'icon icon--clock text--color-greyShade'}):\n",
    "                # return value\n",
    "                    url_date = date.find('span',{'class':'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "                    url_date = re.sub('\\((.*)\\)', '', url_date)\n",
    "                    url_date = url_date.rstrip()\n",
    "                    each_url_date.append(url_date)\n",
    "                \n",
    "\n",
    "        \n",
    "    except:\n",
    "        each_url_date.append(None)\n",
    "    \n",
    "    \n",
    "    \n",
    "pprint.pprint(each_url_date)\n",
    "pprint.pprint(len(each_url_date))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oct 27º. 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct 27º. 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct 27º. 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date\n",
       "0  oct 27º. 2021\n",
       "1  oct 27º. 2021\n",
       "2  oct 27º. 2021"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(each_url_date, columns = ['Date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# each_url_origin = []\n",
    "\n",
    "# for urls in arr_url[8050:8060]:\n",
    "#     print(urls)\n",
    "    \n",
    "\n",
    "#     #bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----)\n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#         }\n",
    "    \n",
    "    \n",
    "#     # request page and soupify\n",
    "#     r = requests.get(urls, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "   \n",
    "               \n",
    "\n",
    "        \n",
    "\n",
    "#     elements = soup.find_all('span',{'class' : 'flex--toW3 overflow--wrap-off text--color-greyShade'})\n",
    "#     print(len(elements))\n",
    "\n",
    "    \n",
    "\n",
    "#     # temporary list\n",
    "#     temp_origin_list = []\n",
    "#     # counter\n",
    "#     count = 0\n",
    "#     while count < len(elements):\n",
    "        \n",
    "#         # loop html elements in elements variable\n",
    "#         for origins in elements:\n",
    "            \n",
    "#             # check for icon, return text and increment count\n",
    "#             if origins.find('svg',{'class' : 'icon--world'}):\n",
    "#                 icon_text = origins.find('span',{'class' : 'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "#                 # return value if it is there and clean string with regex\n",
    "#                 icon_text = re.sub('(Se envía de )', '', icon_text)\n",
    "#                 if len(temp_origin_list) == 0:\n",
    "#                     temp_origin_list.append(icon_text)\n",
    "#                 else:\n",
    "#                     if len(temp_origin_list) == 1:\n",
    "#                         continue\n",
    "                \n",
    "#                 count += 1\n",
    "                \n",
    "#             # check for icon, return text and increment count\n",
    "#             elif origins.find('svg',{'class' : 'icon--location'}):\n",
    "#                 icon_text = origins.find('span',{'class' : 'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "#                 # return value if it is there and clean string with regex\n",
    "#                 icon_text = re.sub('(Se envía de )', '', icon_text)\n",
    "#                 if len(temp_origin_list) == 0:\n",
    "#                     temp_origin_list.append(icon_text)\n",
    "#                 else:\n",
    "#                     if len(temp_origin_list) == 1:\n",
    "#                         continue\n",
    "            \n",
    "                \n",
    "#                 count += 1\n",
    "\n",
    "#             else:\n",
    "#                 count += 1\n",
    "#     # if the list is empty, append None to ensure df array lengths match  \n",
    "#     if len(temp_origin_list) == 0:\n",
    "#         temp_origin_list.append(None)\n",
    "#     # Append temporary list input to 'each_url_origin list' (should be len(1))\n",
    "#     for item in temp_origin_list:\n",
    "#         each_url_origin.append(item)\n",
    "        \n",
    "# pprint.pprint(each_url_origin)\n",
    "# pprint.pprint(len(each_url_origin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each_url_category_1 = []\n",
    "# each_url_category_2 = []\n",
    "# each_url_category_3 = []\n",
    "# each_url_category_4 = []\n",
    "# each_url_category_5 = []\n",
    "# each_url_category_6 = []\n",
    "# each_url_category_7 = []\n",
    "# each_url_category_8 = []\n",
    "# each_url_category_9 = []\n",
    "\n",
    "\n",
    "# for urls in arr_url[170:180]:\n",
    "#     print(urls)\n",
    "    \n",
    "\n",
    "#     #bypass HTTP security & login ( this can't be used for other users - EXPLATION followed-----)\n",
    "\n",
    "#     headers = {\n",
    "#         'authority': 'www.promodescuentos.com',\n",
    "#         'cache-control': 'max-age=0',\n",
    "#         'sec-ch-ua': '\"Google Chrome\";v=\"95\", \"Chromium\";v=\"95\", \";Not A Brand\";v=\"99\"',\n",
    "#         'sec-ch-ua-mobile': '?0',\n",
    "#         'sec-ch-ua-platform': '\"macOS\"',\n",
    "#         'upgrade-insecure-requests': '1',\n",
    "#         'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36',\n",
    "#         'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "#         'sec-fetch-site': 'none',\n",
    "#         'sec-fetch-mode': 'navigate',\n",
    "#         'sec-fetch-user': '?1',\n",
    "#         'sec-fetch-dest': 'document',\n",
    "#         'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "#         'cookie': 'view_layout_horizontal=%221-1%22; show_my_tab=0; f_v=%229f0ea980-3230-11ec-a29c-0242ac110003%22; _ga=GA1.3.1054458069.1634794497; _gid=GA1.3.1552499565.1634794497; ab.storage.userId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%22browser-1626960373888-6%22%2C%22c%22%3A1634794497594%2C%22l%22%3A1634794497605%7D; ab.storage.deviceId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%2264eea135-5993-3f15-ebc4-09adf427628c%22%2C%22c%22%3A1634794497609%2C%22l%22%3A1634794497609%7D; discussions_widget_selected_option=%22popular%22; _hjid=ae0f1ed2-2d92-4f09-bf43-c7bc66931033; __gads=ID=0ce43e3eff6da5ec:T=1634794499:S=ALNI_Mad7QJuOXppxRjU5egRVkmABAHc-A; stg_returning_visitor=Thu%2C%2021%20Oct%202021%2005:35:35%20GMT; navi=%7B%22homepage%22%3A%22picked%22%7D; _hjIncludedInSessionSample=0; xsrf_t=%22HEKJhu3kbDLqi5JfV1bDT2SpB0casC7t8lYr123B%22; _hjAbsoluteSessionInProgress=0; _pk_ses.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=*; stg_externalReferrer=; stg_traffic_source_priority=1; browser_push_permission_requested=1634922729; _gat=1; pepper_session=%22O3fygKnag0an5mcXzMUY4Cte4ZhqyaiBdI8DDjVk%22; remember_6fc0f483e7f442dc50848060ae780d66=%22778370%7CXrIutHkF0kW4HvN6kagIuDIqsQmOzP4HwyizQpKc3jl642wfYMc55YZfHmph%7C%242y%2410%24UCA2KfcAHkp28h5luvz69eim1o4ljCHTkbPNiE.Gm%5C%2FdRld.JuV4ei%22; u_l=1; ab.storage.sessionId.7af503ae-0c84-478f-98b0-ecfff5d67750=%7B%22g%22%3A%226b6610eb-9a5d-5719-499c-6571b7fa98c8%22%2C%22e%22%3A2134922914462%2C%22c%22%3A1634794497601%2C%22l%22%3A1634922914462%7D; stg_last_interaction=Fri%2C%2022%20Oct%202021%2017:15:17%20GMT; _pk_id.12dffd1a-d9f7-4108-953d-b1f490724bce.09fe=6f429fab6ac98163.1634794500.12.1634922917.1634919759.',\n",
    "#         }\n",
    "    \n",
    "    \n",
    "#     # request page and soupify\n",
    "#     r = requests.get(urls, headers=headers).text\n",
    "#     soup = BeautifulSoup(r, 'html.parser')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     # check for categories\n",
    "#     categories = soup.find_all(\"a\",{\"class\" : \"text--b linkPlain cept-thread-group-name thread-link mute--text space--mr-3\"})\n",
    "#     try:\n",
    "        \n",
    "#         tags = []\n",
    "#         for category in categories:\n",
    "#             tags.append(category.text)\n",
    "        \n",
    "#         if len(tags) == 0:\n",
    "\n",
    "#             each_url_category_1.append(None)\n",
    "#             each_url_category_2.append(None)\n",
    "#             each_url_category_3.append(None)\n",
    "#             each_url_category_4.append(None)\n",
    "#             each_url_category_5.append(None)\n",
    "#             each_url_category_6.append(None)\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "#         if len(tags) == 1:\n",
    "\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(None)\n",
    "#             each_url_category_3.append(None)\n",
    "#             each_url_category_4.append(None)\n",
    "#             each_url_category_5.append(None)\n",
    "#             each_url_category_6.append(None)\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#         if len(tags) == 2:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(None)\n",
    "#             each_url_category_4.append(None)\n",
    "#             each_url_category_5.append(None)\n",
    "#             each_url_category_6.append(None)\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#         if len(tags) == 3:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(tags[2])\n",
    "#             each_url_category_4.append(None)\n",
    "#             each_url_category_5.append(None)\n",
    "#             each_url_category_6.append(None)\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#         if len(tags) == 4:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(tags[2])\n",
    "#             each_url_category_4.append(tags[3])\n",
    "#             each_url_category_5.append(None)\n",
    "#             each_url_category_6.append(None)\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#         if len(tags) == 5:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(tags[2])\n",
    "#             each_url_category_4.append(tags[3])\n",
    "#             each_url_category_5.append(tags[4])\n",
    "#             each_url_category_6.append(None)\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#         if len(tags) == 6:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(tags[2])\n",
    "#             each_url_category_4.append(tags[3])\n",
    "#             each_url_category_5.append(tags[4])\n",
    "#             each_url_category_6.append(tags[5])\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#         if len(tags) == 7:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(tags[2])\n",
    "#             each_url_category_4.append(tags[3])\n",
    "#             each_url_category_5.append(tags[4])\n",
    "#             each_url_category_6.append(tags[5])\n",
    "#             each_url_category_7.append(tags[6])\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#         if len(tags) == 8:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(tags[2])\n",
    "#             each_url_category_4.append(tags[3])\n",
    "#             each_url_category_5.append(tags[4])\n",
    "#             each_url_category_6.append(tags[5])\n",
    "#             each_url_category_7.append(tags[6])\n",
    "#             each_url_category_8.append(tags[7])\n",
    "#             each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "\n",
    "#         if len(tags) >= 9:\n",
    "#             each_url_category_1.append(tags[0])\n",
    "#             each_url_category_2.append(tags[1])\n",
    "#             each_url_category_3.append(tags[2])\n",
    "#             each_url_category_4.append(tags[3])\n",
    "#             each_url_category_5.append(tags[4])\n",
    "#             each_url_category_6.append(tags[5])\n",
    "#             each_url_category_7.append(tags[6])\n",
    "#             each_url_category_8.append(tags[7])\n",
    "#             each_url_category_9.append(tags[8])\n",
    "\n",
    "#     except:\n",
    "#         each_url_category_1.append(None)\n",
    "#         each_url_category_2.append(None)\n",
    "#         each_url_category_3.append(None)\n",
    "#         each_url_category_4.append(None)\n",
    "#         each_url_category_5.append(None)\n",
    "#         each_url_category_6.append(None)\n",
    "#         each_url_category_7.append(None)\n",
    "#         each_url_category_8.append(None)\n",
    "#         each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "# print(len(each_url_category_1))\n",
    "# print(len(each_url_category_2))\n",
    "# print(len(each_url_category_3))\n",
    "# print(len(each_url_category_4))\n",
    "# print(len(each_url_category_5))\n",
    "# print(len(each_url_category_6))\n",
    "# print(len(each_url_category_7))\n",
    "# print(len(each_url_category_8))\n",
    "# print(len(each_url_category_9))\n",
    "\n",
    "# print(each_url_category_1)\n",
    "# print(each_url_category_2)\n",
    "# print(each_url_category_3)\n",
    "# print(each_url_category_4)\n",
    "# print(each_url_category_5)\n",
    "# print(each_url_category_6)\n",
    "# print(each_url_category_7)\n",
    "# print(each_url_category_8)\n",
    "# print(each_url_category_9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all code compiled together \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # TOP COMMENTS + THUMBS UP + USER coming later as it requires selenium\n",
    "\n",
    "# # time test\n",
    "# start = time.process_time()\n",
    "\n",
    "# #lists will become columns in dataframe\n",
    "\n",
    "# each_url_degrees = []\n",
    "# each_url_product = []\n",
    "# each_url_final_price = []\n",
    "# each_url_original_price = []\n",
    "# each_url_free_shipping = []\n",
    "# each_url_merchant = []\n",
    "# each_url_username = []\n",
    "# each_url_date = []\n",
    "# each_url_origin = []\n",
    "# url = []\n",
    "# each_url_category_1 = []\n",
    "# each_url_category_2 = []\n",
    "# each_url_category_3 = []\n",
    "# each_url_category_4 = []\n",
    "# each_url_category_5 = []\n",
    "# each_url_category_6 = []\n",
    "# each_url_category_7 = []\n",
    "# each_url_category_8 = []\n",
    "# each_url_category_9 = []\n",
    "# top_comment_user = []\n",
    "# top_comment = []\n",
    "# thumbs_up = []\n",
    "\n",
    "# count = 0\n",
    "# for urls in arr_url[500:510]:\n",
    "#     print(urls)\n",
    "#     iteration_start = time.process_time()\n",
    "#     try:\n",
    "\n",
    "#         DRIVER_PATH = '/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/chromedriver'\n",
    "#         # add headless mode\n",
    "#         options = Options()\n",
    "#         options.add_argument(\"--headless\") # Runs Chrome in headless mode.\n",
    "#         options.add_argument('--no-sandbox') # Bypass OS security model\n",
    "#         driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "#         driver.get(urls)\n",
    "   \n",
    "#         r = driver.page_source\n",
    "#         soup = BeautifulSoup(r, 'html.parser')\n",
    "#      #--------------------------------------------------------------------------------------------------------------------#   \n",
    "#     # append URL to list\n",
    "\n",
    "#         try:\n",
    "#             url.append(urls)\n",
    "#         except:\n",
    "#             url.append(None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#              \n",
    "#     #loop through URLS and obtain degree integer  \n",
    "#         try:\n",
    "#             degrees = soup.find('div',{'class' : \"cept-vote-box\"}).text\n",
    "#             degrees = re.sub('[^-?0-9]', '', degrees)\n",
    "#             # return only digits\n",
    "#             each_url_degrees.append(int(degrees))\n",
    "#         except:\n",
    "#             each_url_degrees.append(None)\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#    \n",
    "#     #loop through URLS and obtain product data\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             product = soup.find('span',{'class' : \"thread-title--item\"}).text\n",
    "#             each_url_product.append(product)\n",
    "#         except:\n",
    "#              each_url_product.append(None)\n",
    "\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#\n",
    "#     # loop through URLS and obtain final price\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             final_price = soup.find('span',{'class' : \"cept-tp\"}).text\n",
    "#             substring = 'GRATIS'\n",
    "#             # check if final price is GRATIS - return 0 if so\n",
    "#             if substring in final_price:\n",
    "#                 each_url_final_price.append(0)\n",
    "#             else:\n",
    "#                 # return only digits (take out special characters to ensure int datatype)\n",
    "#                 final_price = re.sub('[^0-9]', '', final_price)\n",
    "#                 each_url_final_price.append(int(final_price))\n",
    "#         except:\n",
    "#             each_url_final_price.append(None)\n",
    "\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#     # loop through URLS and obtain original price\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             original_price = soup.find('span',{'class' : \"cept-next-best-price\"}).text\n",
    "#             original_price = re.sub('[^0-9]', '', original_price)\n",
    "#             each_url_original_price.append(int(original_price))\n",
    "#         except:\n",
    "#             each_url_original_price.append(None)\n",
    "\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#\n",
    "#     # loop through URLS and obtain free shipping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         try1 = soup.find('span',{'class' : \"cept-shipping-price\"})\n",
    "#         try2 = soup.find('span',{'class' : \"cept-tp\"})\n",
    "#         try3 = soup.find('div',{ 'class' : 'threadItem-title'})\n",
    "#         try4 = soup.find('span',{'class' : 'overflow--fade'})\n",
    "#         try5 = soup.find('span',{'class' : 'text--color-greyShade'})\n",
    "\n",
    "#         # check for envio gratis and return true\n",
    "#         substring1 = 'Envío gratis'\n",
    "#         try:\n",
    "\n",
    "#             if (try1 is not None) and (substring1 in try1.text):\n",
    "#             # append to list\n",
    "#                 each_url_free_shipping.append(True)\n",
    "#             elif (try2 is not None) and (substring1 in try2.text):\n",
    "#                 each_url_free_shipping.append(True) \n",
    "#             elif (try3 is not None) and (substring1 in try3.text):\n",
    "#                 each_url_free_shipping.append(True)\n",
    "#             elif (try4 is not None) and (substring1 in try4.text):\n",
    "#                 each_url_free_shipping.append(True)\n",
    "#             elif (try5 is not None) and (substring1 in try5.text):\n",
    "#                 each_url_free_shipping.append(True)\n",
    "#             else:\n",
    "#                 each_url_free_shipping.append(False)\n",
    "\n",
    "#         except:\n",
    "#             each_url_free_shipping.append(False)\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#\n",
    "#     # loop through URLS and obtain merchant\n",
    "\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             merchant = soup.find('span',{'class' : \"cept-merchant-name\"}).text\n",
    "#             merchant = re.sub('[\\s*$]', '', merchant)\n",
    "\n",
    "\n",
    "#             each_url_merchant.append(merchant)\n",
    "#         except:\n",
    "#             each_url_merchant.append(None)\n",
    "\n",
    "\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#     # loop through URLS and obtain usernames\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             username = soup.find('span',{'class' : \"thread-username\"}).text\n",
    "#             username = re.sub('[\\s*$]', '', username)\n",
    "#             each_url_username.append(username)\n",
    "#         except:\n",
    "#             each_url_username.append(None)\n",
    "\n",
    "\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#         # loop through URLS and obtain date\n",
    "\n",
    "#         try:\n",
    "        \n",
    "#             # check for post date \n",
    "#             dates = soup.find_all('span',{'class' : \"flex--toW3 overflow--wrap-off text--color-greyShade\"})\n",
    "#             print(len(dates))\n",
    "#             print(dates)\n",
    "\n",
    "#             if len(dates) == 0:\n",
    "#                 # check for publication\n",
    "#                 pubs = soup.find('span', {'class' :'flex--toW3 overflow--wrap-off text--color-greyShade text--b'})\n",
    "#                 # if no publication date add None to the dates list/array\n",
    "#                 if len(pubs) == 0:\n",
    "#                     each_url_date.append(None)\n",
    "#                     # if there is a publication date\n",
    "#                 else:\n",
    "#                     # return the date inside the brackets using regex and add to the dates list/array\n",
    "#                     pub_date = pubs.find('span', {'class' :'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "#                     regex = re.findall('\\((.*)\\)', pub_date)\n",
    "#                     pub_date = regex[0]\n",
    "#                     pub_date = re.sub('(Publicado)', '', pub_date)\n",
    "\n",
    "#                     # check for string year\n",
    "#                     substring_year = '20'\n",
    "#                     # current year\n",
    "#                     current_year = str(datetime.datetime.now().year)\n",
    "#                     # if it does have the year, don't append \n",
    "#                     if substring_year in pub_date:\n",
    "#                         pub_date = re.sub('^[ \\t]+', '', pub_date)\n",
    "#                         each_url_date.append(pub_date)\n",
    "#                     else:\n",
    "#                         # if it does, concatenate year\n",
    "#                         pub_date = re.sub('^[ \\t]+', '', pub_date) + ' ' +  current_year\n",
    "#                         each_url_date.append(pub_date)\n",
    "\n",
    "\n",
    "#             elif soup.find('span', {'class' :'flex--toW3 overflow--wrap-off text--color-greyShade text--b'}):\n",
    "#                 pubs = soup.find('span', {'class' :'flex--toW3 overflow--wrap-off text--color-greyShade text--b'})\n",
    "\n",
    "#                 # if no publication date add None to the dates list/array\n",
    "#                 if len(pubs) == 0:\n",
    "#                     each_url_date.append(None)\n",
    "#                     # if there is a publication date\n",
    "#                 else:\n",
    "#                     # return the date inside the brackets using regex and add to the dates list/array\n",
    "#                     pub_date = pubs.find('span', {'class' :'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "#                     regex = re.findall('\\((.*)\\)', pub_date)\n",
    "#                     pub_date = regex[0]\n",
    "#                     pub_date = re.sub('(Publicado)', '', pub_date)\n",
    "\n",
    "#                     # check for string year\n",
    "#                     substring_year = '20'\n",
    "#                     # current year\n",
    "#                     current_year = str(datetime.datetime.now().year)\n",
    "#                     # if it does have the year, don't append \n",
    "#                     if substring_year in pub_date:\n",
    "#                         pub_date = re.sub('^[ \\t]+', '', pub_date)\n",
    "#                         each_url_date.append(pub_date)\n",
    "#                     else:\n",
    "#                         # if it does, concatenate year\n",
    "#                         pub_date = re.sub('^[ \\t]+', '', pub_date) + ' ' + current_year\n",
    "#                         each_url_date.append(pub_date)\n",
    "\n",
    "\n",
    "\n",
    "#             else:\n",
    "#                 # loop through the classes\n",
    "#                 for date in dates:\n",
    "#                 # check for clock icon\n",
    "#                     if date.find('svg',{'class':'icon icon--clock text--color-greyShade'}):\n",
    "#                     # return value\n",
    "#                         url_date = date.find('span',{'class':'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "#                         each_url_date.append(url_date)\n",
    "\n",
    "\n",
    "\n",
    "#         except:\n",
    "#             each_url_date.append(None)\n",
    "\n",
    "\n",
    "\n",
    "#     #--------------------------------------------------------------------------------------------------------------------#    \n",
    "\n",
    "#         #obtain origin \n",
    "\n",
    "        \n",
    "#         try:\n",
    "#             elements = soup.find_all('span',{'class' : 'flex--toW3 overflow--wrap-off text--color-greyShade'})\n",
    "    \n",
    "#             # temporary list\n",
    "#             temp_origin_list = []\n",
    "#             # counter\n",
    "#             count = 0\n",
    "#             while count < len(elements):\n",
    "\n",
    "#                 # loop html elements in elements variable\n",
    "#                 for origins in elements:\n",
    "\n",
    "#                     # check for icon, return text and increment count\n",
    "#                     if origins.find('svg',{'class' : 'icon--world'}):\n",
    "#                         icon_text = origins.find('span',{'class' : 'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "#                         # return value if it is there and clean string with regex\n",
    "#                         icon_text = re.sub('(Se envía de )', '', icon_text)\n",
    "#                         if len(temp_origin_list) == 0:\n",
    "#                             temp_origin_list.append(icon_text)\n",
    "#                         else:\n",
    "#                             if len(temp_origin_list) == 1:\n",
    "#                                 continue\n",
    "\n",
    "#                         count += 1\n",
    "\n",
    "#                     # check for icon, return text and increment count\n",
    "#                     elif origins.find('svg',{'class' : 'icon--location'}):\n",
    "#                         icon_text = origins.find('span',{'class' : 'space--fromW3-ml-1 size--all-s space--t-2 space--fromW3-t-0 overflow--wrap-on space--fromW3-r-2'}).text\n",
    "#                         # return value if it is there and clean string with regex\n",
    "#                         icon_text = re.sub('(Se envía de )', '', icon_text)\n",
    "#                         if len(temp_origin_list) == 0:\n",
    "#                             temp_origin_list.append(icon_text)\n",
    "#                         else:\n",
    "#                             if len(temp_origin_list) == 1:\n",
    "#                                 continue\n",
    "\n",
    "\n",
    "#                         count += 1\n",
    "\n",
    "#                     else:\n",
    "#                         count += 1\n",
    "#             # if the list is empty, append None to ensure df array lengths match  \n",
    "#             if len(temp_origin_list) == 0:\n",
    "#                 temp_origin_list.append(None)\n",
    "#             # Append temporary list input to 'each_url_origin list' (should be len(1))\n",
    "#             for item in temp_origin_list:\n",
    "#                 each_url_origin.append(item)\n",
    "        \n",
    "#         except:\n",
    "#             each_url_origin.append(None)\n",
    "\n",
    "\n",
    "#      #--------------------------------------------------------------------------------------------------------------------#   \n",
    "\n",
    "\n",
    "#         # check for categories\n",
    "#         categories = soup.find_all(\"a\",{\"class\" : \"text--b linkPlain cept-thread-group-name thread-link mute--text space--mr-3\"})\n",
    "#         try:\n",
    "\n",
    "#             tags = []\n",
    "#             for category in categories:\n",
    "#                 tags.append(category.text)\n",
    "\n",
    "#             if len(tags) == 0:\n",
    "\n",
    "#                 each_url_category_1.append(None)\n",
    "#                 each_url_category_2.append(None)\n",
    "#                 each_url_category_3.append(None)\n",
    "#                 each_url_category_4.append(None)\n",
    "#                 each_url_category_5.append(None)\n",
    "#                 each_url_category_6.append(None)\n",
    "#                 each_url_category_7.append(None)\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "#             if len(tags) == 1:\n",
    "\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(None)\n",
    "#                 each_url_category_3.append(None)\n",
    "#                 each_url_category_4.append(None)\n",
    "#                 each_url_category_5.append(None)\n",
    "#                 each_url_category_6.append(None)\n",
    "#                 each_url_category_7.append(None)\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             if len(tags) == 2:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(None)\n",
    "#                 each_url_category_4.append(None)\n",
    "#                 each_url_category_5.append(None)\n",
    "#                 each_url_category_6.append(None)\n",
    "#                 each_url_category_7.append(None)\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             if len(tags) == 3:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(tags[2])\n",
    "#                 each_url_category_4.append(None)\n",
    "#                 each_url_category_5.append(None)\n",
    "#                 each_url_category_6.append(None)\n",
    "#                 each_url_category_7.append(None)\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             if len(tags) == 4:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(tags[2])\n",
    "#                 each_url_category_4.append(tags[3])\n",
    "#                 each_url_category_5.append(None)\n",
    "#                 each_url_category_6.append(None)\n",
    "#                 each_url_category_7.append(None)\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             if len(tags) == 5:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(tags[2])\n",
    "#                 each_url_category_4.append(tags[3])\n",
    "#                 each_url_category_5.append(tags[4])\n",
    "#                 each_url_category_6.append(None)\n",
    "#                 each_url_category_7.append(None)\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             if len(tags) == 6:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(tags[2])\n",
    "#                 each_url_category_4.append(tags[3])\n",
    "#                 each_url_category_5.append(tags[4])\n",
    "#                 each_url_category_6.append(tags[5])\n",
    "#                 each_url_category_7.append(None)\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             if len(tags) == 7:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(tags[2])\n",
    "#                 each_url_category_4.append(tags[3])\n",
    "#                 each_url_category_5.append(tags[4])\n",
    "#                 each_url_category_6.append(tags[5])\n",
    "#                 each_url_category_7.append(tags[6])\n",
    "#                 each_url_category_8.append(None)\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             if len(tags) == 8:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(tags[2])\n",
    "#                 each_url_category_4.append(tags[3])\n",
    "#                 each_url_category_5.append(tags[4])\n",
    "#                 each_url_category_6.append(tags[5])\n",
    "#                 each_url_category_7.append(tags[6])\n",
    "#                 each_url_category_8.append(tags[7])\n",
    "#                 each_url_category_9.append(None)\n",
    "\n",
    "\n",
    "#             # in case any posts have more than 9 tags/categories\n",
    "#             if len(tags) >= 9:\n",
    "#                 each_url_category_1.append(tags[0])\n",
    "#                 each_url_category_2.append(tags[1])\n",
    "#                 each_url_category_3.append(tags[2])\n",
    "#                 each_url_category_4.append(tags[3])\n",
    "#                 each_url_category_5.append(tags[4])\n",
    "#                 each_url_category_6.append(tags[5])\n",
    "#                 each_url_category_7.append(tags[6])\n",
    "#                 each_url_category_8.append(tags[7])\n",
    "#                 each_url_category_9.append(tags[8])\n",
    "\n",
    "#         except:\n",
    "#             each_url_category_1.append(None)\n",
    "#             each_url_category_2.append(None)\n",
    "#             each_url_category_3.append(None)\n",
    "#             each_url_category_4.append(None)\n",
    "#             each_url_category_5.append(None)\n",
    "#             each_url_category_6.append(None)\n",
    "#             each_url_category_7.append(None)\n",
    "#             each_url_category_8.append(None)\n",
    "#             each_url_category_9.append(None)\n",
    "            \n",
    "# #--------------------------------------------------------------------------------------------------------------------#   \n",
    "\n",
    "#         # check for top comment, username and thumbs up\n",
    "#         try:\n",
    "            \n",
    "#             if soup.find_all('span',{'class':'lbox--v-3 space--l-2 size--all-m size--fromW2-l text--b'}):\n",
    "#                 find_comments = soup.find_all('span',{'class':'lbox--v-3 space--l-2 size--all-m size--fromW2-l text--b'})\n",
    "#                 for elements in find_comments:\n",
    "#                     # check for top comments\n",
    "#                     if 'Mejores comentarios' in elements.text:\n",
    "#                         # if there is, find the username (first matching element) and append to list\n",
    "#                         if soup.find('span',{'class': 'userInfo-username'}).text:\n",
    "#                             user_name = soup.find('span',{'class': 'userInfo-username'}).text\n",
    "#                             top_comment_user.append(user_name)\n",
    "#                         else:\n",
    "#                             top_comment_user.appned(None)\n",
    "\n",
    "#                         # check for thumbs up amount and append to list\n",
    "#                         if soup.find('span', {'class': 'comment-like'}).text:\n",
    "#                             thumbs_up_count = soup.find('span', {'class': 'comment-like'}).text\n",
    "#                             thumbs_up.append(thumbs_up_count)\n",
    "#                         else:\n",
    "#                             thumbs_up.append(None)\n",
    "\n",
    "\n",
    "#                         # check for the parent div for comments\n",
    "#                         if soup.find('div',{'class':'commentList-item'}):\n",
    "#                             # assign it to a variable\n",
    "#                             parent = soup.find('div',{'class':'commentList-item'})\n",
    "#                             # check for top comment(first entry)\n",
    "#                             if parent.find('div',{'class':'comment-body'}):\n",
    "#                                 # grab text\n",
    "#                                 comment_text = parent.find('div',{'class':'comment-body'}).text\n",
    "#                                 # if there is no text, it is assumed to be a graphic or image\n",
    "#                                 if comment_text == '':\n",
    "#                                     top_comment.append('Graphic instead of text (image/meme)')\n",
    "#                                     # append text if there is\n",
    "#                                 else:\n",
    "#                                     top_comment.append(comment_text)\n",
    "#                         else:\n",
    "#                             top_comment.append(None)\n",
    "\n",
    "\n",
    "#                     else:\n",
    "#                         top_comment.append(None)\n",
    "\n",
    "#             else:\n",
    "#                 top_comment_user.append(None)\n",
    "#                 top_comment.append(None)\n",
    "#                 thumbs_up.append(None)\n",
    "            \n",
    "#         except:\n",
    "#             top_comment_user.append(None)\n",
    "#             top_comment.append(None)\n",
    "#             thumbs_up.append(None)\n",
    "            \n",
    "            \n",
    "#  #--------------------------------------------------------------------------------------------------------------------#   \n",
    "            \n",
    "\n",
    "#     except:\n",
    "#         each_url_degrees.append(None)\n",
    "#         each_url_product.append(None)\n",
    "#         each_url_final_price.append(None)\n",
    "#         each_url_original_price.append(None)\n",
    "#         each_url_free_shipping.append(None)\n",
    "#         each_url_merchant.append(None)\n",
    "#         each_url_username.append(None)\n",
    "#         each_url_date.append(None)\n",
    "#         each_url_origin.append(None)\n",
    "#         url.append(None)\n",
    "#         each_url_category_1.append(None)\n",
    "#         each_url_category_2.append(None)\n",
    "#         each_url_category_3.append(None)\n",
    "#         each_url_category_4.append(None)\n",
    "#         each_url_category_5.append(None)\n",
    "#         each_url_category_6.append(None)\n",
    "#         each_url_category_7.append(None)\n",
    "#         each_url_category_8.append(None)\n",
    "#         each_url_category_9.append(None)\n",
    "#         top_comment_user.append(None)\n",
    "#         top_comment.append(None)\n",
    "#         thumbs_up.append(None)\n",
    "            \n",
    "            \n",
    "        #if(count%500==0):\n",
    "#         data_dict = {'Degrees':each_url_degrees,'Product':each_url_product,'Final_Price':each_url_final_price,'Original_Price':each_url_original_price,'Free_Shipping':each_url_free_shipping,'Merchant':each_url_merchant, 'Username':each_url_username,'Date':each_url_date,'Origin':each_url_origin,'URL':url, 'Category_1':each_url_category_1,'Category_2':each_url_category_2,'Category_3':each_url_category_3,'Category_4':each_url_category_4,'Category_5':each_url_category_5,'Category_6':each_url_category_6,'Category_7':each_url_category_7,'Category_8':each_url_category_8,'Category_9':each_url_category_9,'top_comment_user':top_comment_user,'top_comment':top_comment,'thumbs_up':thumbs_up}\n",
    "#         neuvas_data = pd.DataFrame.from_dict(data_dict)\n",
    "#         nuevas_data.to_csv(\"/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/nuevas_data.csv\", index = False)\n",
    "#     count += 1\n",
    "#     print(time.process_time() - iteration_start)\n",
    "# # code run time\n",
    "\n",
    "# print(time.process_time() - start)\n",
    "\n",
    "\n",
    "# data_dict = {'Degrees':each_url_degrees,'Product':each_url_product,'Final_Price':each_url_final_price,'Original_Price':each_url_original_price,'Free_Shipping':each_url_free_shipping,'Merchant':each_url_merchant, 'Username':each_url_username,'Date':each_url_date,'Origin':each_url_origin,'URL':url, 'Category_1':each_url_category_1,'Category_2':each_url_category_2,'Category_3':each_url_category_3,'Category_4':each_url_category_4,'Category_5':each_url_category_5,'Category_6':each_url_category_6,'Category_7':each_url_category_7,'Category_8':each_url_category_8,'Category_9':each_url_category_9,'top_comment_user':top_comment_user,'top_comment':top_comment,'thumbs_up':thumbs_up}\n",
    "# nuevas_data = pd.DataFrame.from_dict(data_dict)\n",
    "# nuevas_data.to_csv(\"/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/nuevas_data.csv\", index = False)\n",
    "#                                                    #-END-#\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevas_data = pd.read_csv(\"/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/nuevas_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # used to debug when array lengths aren't the same. Pinpoint which data point is causing the error.\n",
    "\n",
    "# print(len(each_url_degrees))\n",
    "# print(len(each_url_product))\n",
    "# print(len(each_url_final_price))\n",
    "# print(len(each_url_original_price))\n",
    "# print(len(each_url_free_shipping))\n",
    "# print(len(each_url_merchant))\n",
    "# print(len(each_url_username))\n",
    "# print(len(each_url_date))\n",
    "# print(len(each_url_origin))\n",
    "# print(len(url))\n",
    "# print(len(each_url_category_1))\n",
    "# print(len(each_url_category_2))\n",
    "# print(len(each_url_category_3))\n",
    "# print(len(each_url_category_4))\n",
    "# print(len(each_url_category_5))\n",
    "# print(len(each_url_category_6))\n",
    "# print(len(each_url_category_7))\n",
    "# print(len(each_url_category_8))\n",
    "# print(len(each_url_category_9))\n",
    "# print(len(top_comment_user))\n",
    "# print(len(top_comment))\n",
    "# print(len(thumbs_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a dictionary with data lists\n",
    "# data_dict = {'Degrees':each_url_degrees,'Product':each_url_product,'Final_Price':each_url_final_price,'Original_Price':each_url_original_price,'Free_Shipping':each_url_free_shipping,'Merchant':each_url_merchant, 'Username':each_url_username,'Date':each_url_date,'Origin':each_url_origin,'URL':url, 'Category_1':each_url_category_1,'Category_2':each_url_category_2,'Category_3':each_url_category_3,'Category_4':each_url_category_4,'Category_5':each_url_category_5,'Category_6':each_url_category_6,'Category_7':each_url_category_7,'Category_8':each_url_category_8,'Category_9':each_url_category_9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use dictionary to create a dataframe (3d array)\n",
    "# df_scraped_data = pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degrees</th>\n",
       "      <th>Product</th>\n",
       "      <th>Final_Price</th>\n",
       "      <th>Original_Price</th>\n",
       "      <th>Free_Shipping</th>\n",
       "      <th>Merchant</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Origin</th>\n",
       "      <th>URL</th>\n",
       "      <th>...</th>\n",
       "      <th>Category_3</th>\n",
       "      <th>Category_4</th>\n",
       "      <th>Category_5</th>\n",
       "      <th>Category_6</th>\n",
       "      <th>Category_7</th>\n",
       "      <th>Category_8</th>\n",
       "      <th>Category_9</th>\n",
       "      <th>top_comment_user</th>\n",
       "      <th>top_comment</th>\n",
       "      <th>thumbs_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-139</td>\n",
       "      <td>Amazon Outriders Day One Edition - PlayStation 5</td>\n",
       "      <td>469.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>RobertValencia</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/outrid...</td>\n",
       "      <td>...</td>\n",
       "      <td>Juegos de PS5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305</td>\n",
       "      <td>Sams Club Homero yogurth griego chobani</td>\n",
       "      <td>40.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Sam'sClub</td>\n",
       "      <td>andres18torres</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>Local</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/sams-c...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>Solana: Papel Higiénico Elite</td>\n",
       "      <td>17.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Soriana</td>\n",
       "      <td>Depredador</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/papel-...</td>\n",
       "      <td>...</td>\n",
       "      <td>Papel higiénico</td>\n",
       "      <td>Productos para el cuidado íntimo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-247</td>\n",
       "      <td>Alíenexpress: Adornos Navideños Creativos... m...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>palio007</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/adorno...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monab</td>\n",
       "      <td>Están divertidos!!!      .   Y hay muchos espa...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185</td>\n",
       "      <td>Soriana: Avena Gerber 270gr</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Soriana</td>\n",
       "      <td>Depredador</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/avena-...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>412</td>\n",
       "      <td>Amazon Control alámbrico blanco Y Negro para X...</td>\n",
       "      <td>605.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>ELPROBADORPRO</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/contro...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tlatuani</td>\n",
       "      <td>Tomado de las reseñas de Amazon Me tomo la mol...</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47</td>\n",
       "      <td>Chedraui: 25% de descuento Bebidas Seltzer Hel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Chedraui</td>\n",
       "      <td>vlma</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/chedra...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>535</td>\n",
       "      <td>WALMART/LAVADORA MABE 21 KG ÚLTIMA LIQUIDACIÓN...</td>\n",
       "      <td>5270.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>CHENTE.</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/walmar...</td>\n",
       "      <td>...</td>\n",
       "      <td>Lavadoras</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oscar_HurTa</td>\n",
       "      <td>Con eso te compras una lavadora gamer jajajaja...</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>Amazon: ADATA Classic Series C008 32 GB USB 2....</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Brayan_SanchezLopez</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/adata-...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>Chedraui: 3 x 2 en Vitamina C Supramed 10 sobr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Chedraui</td>\n",
       "      <td>vlma</td>\n",
       "      <td>oct 24º. 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/chedra...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Degrees                                            Product  Final_Price  \\\n",
       "0     -139   Amazon Outriders Day One Edition - PlayStation 5        469.0   \n",
       "1      305            Sams Club Homero yogurth griego chobani         40.0   \n",
       "2       91                      Solana: Papel Higiénico Elite         17.0   \n",
       "3     -247  Alíenexpress: Adornos Navideños Creativos... m...         33.0   \n",
       "4      185                        Soriana: Avena Gerber 270gr          5.0   \n",
       "5      412  Amazon Control alámbrico blanco Y Negro para X...        605.0   \n",
       "6       47  Chedraui: 25% de descuento Bebidas Seltzer Hel...          NaN   \n",
       "7      535  WALMART/LAVADORA MABE 21 KG ÚLTIMA LIQUIDACIÓN...       5270.0   \n",
       "8       43  Amazon: ADATA Classic Series C008 32 GB USB 2....         68.0   \n",
       "9       30  Chedraui: 3 x 2 en Vitamina C Supramed 10 sobr...          NaN   \n",
       "\n",
       "   Original_Price  Free_Shipping   Merchant             Username  \\\n",
       "0             NaN           True     Amazon       RobertValencia   \n",
       "1           150.0          False  Sam'sClub       andres18torres   \n",
       "2            34.0          False    Soriana           Depredador   \n",
       "3             NaN          False        NaN             palio007   \n",
       "4            45.0          False    Soriana           Depredador   \n",
       "5          1099.0           True     Amazon        ELPROBADORPRO   \n",
       "6             NaN          False   Chedraui                 vlma   \n",
       "7             NaN          False    Walmart              CHENTE.   \n",
       "8             NaN           True     Amazon  Brayan_SanchezLopez   \n",
       "9             NaN          False   Chedraui                 vlma   \n",
       "\n",
       "            Date         Origin  \\\n",
       "0  oct 24º. 2021            NaN   \n",
       "1  oct 24º. 2021          Local   \n",
       "2  oct 24º. 2021            NaN   \n",
       "3  oct 24º. 2021            NaN   \n",
       "4  oct 24º. 2021            NaN   \n",
       "5  oct 24º. 2021  United States   \n",
       "6  oct 24º. 2021            NaN   \n",
       "7  oct 24º. 2021            NaN   \n",
       "8  oct 24º. 2021         Mexico   \n",
       "9  oct 24º. 2021            NaN   \n",
       "\n",
       "                                                 URL  ...       Category_3  \\\n",
       "0  https://www.promodescuentos.com/ofertas/outrid...  ...    Juegos de PS5   \n",
       "1  https://www.promodescuentos.com/ofertas/sams-c...  ...              NaN   \n",
       "2  https://www.promodescuentos.com/ofertas/papel-...  ...  Papel higiénico   \n",
       "3  https://www.promodescuentos.com/ofertas/adorno...  ...              NaN   \n",
       "4  https://www.promodescuentos.com/ofertas/avena-...  ...              NaN   \n",
       "5  https://www.promodescuentos.com/ofertas/contro...  ...              NaN   \n",
       "6  https://www.promodescuentos.com/ofertas/chedra...  ...              NaN   \n",
       "7  https://www.promodescuentos.com/ofertas/walmar...  ...        Lavadoras   \n",
       "8  https://www.promodescuentos.com/ofertas/adata-...  ...              NaN   \n",
       "9  https://www.promodescuentos.com/ofertas/chedra...  ...              NaN   \n",
       "\n",
       "                         Category_4 Category_5 Category_6  Category_7  \\\n",
       "0                               NaN        NaN        NaN         NaN   \n",
       "1                               NaN        NaN        NaN         NaN   \n",
       "2  Productos para el cuidado íntimo        NaN        NaN         NaN   \n",
       "3                               NaN        NaN        NaN         NaN   \n",
       "4                               NaN        NaN        NaN         NaN   \n",
       "5                               NaN        NaN        NaN         NaN   \n",
       "6                               NaN        NaN        NaN         NaN   \n",
       "7                               NaN        NaN        NaN         NaN   \n",
       "8                               NaN        NaN        NaN         NaN   \n",
       "9                               NaN        NaN        NaN         NaN   \n",
       "\n",
       "   Category_8  Category_9  top_comment_user  \\\n",
       "0         NaN         NaN               NaN   \n",
       "1         NaN         NaN               NaN   \n",
       "2         NaN         NaN               NaN   \n",
       "3         NaN         NaN             Monab   \n",
       "4         NaN         NaN               NaN   \n",
       "5         NaN         NaN          tlatuani   \n",
       "6         NaN         NaN               NaN   \n",
       "7         NaN         NaN       Oscar_HurTa   \n",
       "8         NaN         NaN               NaN   \n",
       "9         NaN         NaN               NaN   \n",
       "\n",
       "                                         top_comment thumbs_up  \n",
       "0                                                NaN       NaN  \n",
       "1                                                NaN       NaN  \n",
       "2                                                NaN       NaN  \n",
       "3  Están divertidos!!!      .   Y hay muchos espa...       9.0  \n",
       "4                                                NaN       NaN  \n",
       "5  Tomado de las reseñas de Amazon Me tomo la mol...      81.0  \n",
       "6                                                NaN       NaN  \n",
       "7  Con eso te compras una lavadora gamer jajajaja...      16.0  \n",
       "8                                                NaN       NaN  \n",
       "9                                                NaN       NaN  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show last 10 results to check data integrity\n",
    "nuevas_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the dataframe to csv format\n",
    "# scraped_data.to_csv(\"/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/ric_data_3.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in csv file to clean data\n",
    "# df = pd.read_csv(\"/Users/Niall-McNulty/Desktop/Computer Science Projects:Courses/Web Scraping/Web-scraping-www.promodescuentos.com/ric_data_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Degrees           10 non-null     int64  \n",
      " 1   Product           10 non-null     object \n",
      " 2   Final_Price       8 non-null      float64\n",
      " 3   Original_Price    4 non-null      float64\n",
      " 4   Free_Shipping     10 non-null     bool   \n",
      " 5   Merchant          9 non-null      object \n",
      " 6   Username          10 non-null     object \n",
      " 7   Date              10 non-null     object \n",
      " 8   Origin            3 non-null      object \n",
      " 9   URL               10 non-null     object \n",
      " 10  Category_1        10 non-null     object \n",
      " 11  Category_2        7 non-null      object \n",
      " 12  Category_3        3 non-null      object \n",
      " 13  Category_4        1 non-null      object \n",
      " 14  Category_5        0 non-null      float64\n",
      " 15  Category_6        0 non-null      float64\n",
      " 16  Category_7        0 non-null      float64\n",
      " 17  Category_8        0 non-null      float64\n",
      " 18  Category_9        0 non-null      float64\n",
      " 19  top_comment_user  3 non-null      object \n",
      " 20  top_comment       3 non-null      object \n",
      " 21  thumbs_up         3 non-null      float64\n",
      "dtypes: bool(1), float64(8), int64(1), object(12)\n",
      "memory usage: 1.8+ KB\n"
     ]
    }
   ],
   "source": [
    "nuevas_data.info()\n",
    "nuevas_data['Date'] = nuevas_data['Date'].astype(str)\n",
    "nuevas_data['Free_Shipping'] = nuevas_data['Free_Shipping'].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## top 100 entries for comparing and debug.\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # bottom 100 entries for debug and comparing.\n",
    "# df.tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_correction(col):\n",
    "   \n",
    "    substring_double_year = '2020 2021'\n",
    "    # check for substring\n",
    "    if substring_double_year in str(col):\n",
    "        # split the string\n",
    "        split_column_values = col.split(\" \")\n",
    "        # empty string\n",
    "        date_1_new = ''\n",
    "        \n",
    "        # set a counter\n",
    "        count = 0\n",
    "        # loop through list elements\n",
    "        for x in split_column_values:\n",
    "            if count == 3:\n",
    "                break\n",
    "            # concatenate strings together - except for the last element\n",
    "            else:\n",
    "                date_1_new += (str(x) + ' ')\n",
    "                count += 1\n",
    "\n",
    "        # split the string to erase weird symbol\n",
    "        date_2_new = date_1_new.split(\"º.\")\n",
    "        # join them back together\n",
    "        date_2_new = \",\".join(date_2_new)\n",
    "        # return without trailing white space\n",
    "        return date_2_new.rstrip()\n",
    "        \n",
    "    else:\n",
    "        # split the string to erase weird symbol\n",
    "        new_date =  str(col).split(\"º.\")\n",
    "        # join them back together\n",
    "        new_date_2 = \",\".join(new_date)\n",
    "        # return without trailing white space\n",
    "        return new_date_2\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevas_data['Date'] = nuevas_data['Date'].apply(date_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oct 24, 2021\n",
       "1    oct 24, 2021\n",
       "2    oct 24, 2021\n",
       "3    oct 24, 2021\n",
       "4    oct 24, 2021\n",
       "5    oct 24, 2021\n",
       "6    oct 24, 2021\n",
       "7    oct 24, 2021\n",
       "8    oct 24, 2021\n",
       "9    oct 24, 2021\n",
       "Name: Date, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuevas_data['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].apply(date_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oct 27, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct 27, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct 27, 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date\n",
       "0  oct 27, 2021\n",
       "1  oct 27, 2021\n",
       "2  oct 27, 2021"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_translation(col):\n",
    "    if 'ene' in col:\n",
    "        return col.replace('ene','January')\n",
    "    elif 'feb' in col:\n",
    "        return col.replace('feb','February')\n",
    "    elif 'mar' in col:\n",
    "        return col.replace('mar','March')\n",
    "    elif 'abr' in col:\n",
    "        return col.replace('abr','April')\n",
    "    elif 'may' in col:\n",
    "        return col.replace('may', 'May')\n",
    "    elif 'jun' in col:\n",
    "        return col.replace('jun','June')\n",
    "    elif 'jul' in col:\n",
    "        return col.replace('jul','July')\n",
    "    elif 'ago' in col:\n",
    "        return col.replace('ago','August')\n",
    "    elif 'sep' in col:\n",
    "        return col.replace('sep','September')\n",
    "    elif 'oct' in col:\n",
    "        return col.replace('oct', 'October')\n",
    "    elif 'nov' in col:\n",
    "        return col.replace('nov','November')\n",
    "    elif 'dic' in col:\n",
    "        return col.replace('dic','December')\n",
    "    else:\n",
    "        return col\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].apply(month_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>October 27, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>October 27, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>October 27, 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date\n",
       "0  October 27, 2021\n",
       "1  October 27, 2021\n",
       "2  October 27, 2021"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time(col):\n",
    "    try:\n",
    "        return datetime.strptime(col, \"%B %d, %Y\")\n",
    "    except:\n",
    "        return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].apply(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2021-10-27\n",
       "1   2021-10-27\n",
       "2   2021-10-27\n",
       "Name: Date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date\n",
       "0 2021-10-27\n",
       "1 2021-10-27\n",
       "2 2021-10-27"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix date\n",
    "nuevas_data.Date = nuevas_data.Date.apply(date_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oct 24, 2021\n",
       "1    oct 24, 2021\n",
       "2    oct 24, 2021\n",
       "3    oct 24, 2021\n",
       "4    oct 24, 2021\n",
       "5    oct 24, 2021\n",
       "6    oct 24, 2021\n",
       "7    oct 24, 2021\n",
       "8    oct 24, 2021\n",
       "9    oct 24, 2021\n",
       "Name: Date, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuevas_data.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate month\n",
    "nuevas_data['Date'] = nuevas_data['Date'].apply(month_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply datetime format\n",
    "nuevas_data['Date'] = nuevas_data['Date'].apply(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degrees</th>\n",
       "      <th>Product</th>\n",
       "      <th>Final_Price</th>\n",
       "      <th>Original_Price</th>\n",
       "      <th>Free_Shipping</th>\n",
       "      <th>Merchant</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Origin</th>\n",
       "      <th>URL</th>\n",
       "      <th>...</th>\n",
       "      <th>Category_3</th>\n",
       "      <th>Category_4</th>\n",
       "      <th>Category_5</th>\n",
       "      <th>Category_6</th>\n",
       "      <th>Category_7</th>\n",
       "      <th>Category_8</th>\n",
       "      <th>Category_9</th>\n",
       "      <th>top_comment_user</th>\n",
       "      <th>top_comment</th>\n",
       "      <th>thumbs_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-139</td>\n",
       "      <td>Amazon Outriders Day One Edition - PlayStation 5</td>\n",
       "      <td>469.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>RobertValencia</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/outrid...</td>\n",
       "      <td>...</td>\n",
       "      <td>Juegos de PS5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305</td>\n",
       "      <td>Sams Club Homero yogurth griego chobani</td>\n",
       "      <td>40.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Sam'sClub</td>\n",
       "      <td>andres18torres</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Local</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/sams-c...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>Solana: Papel Higiénico Elite</td>\n",
       "      <td>17.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Soriana</td>\n",
       "      <td>Depredador</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/papel-...</td>\n",
       "      <td>...</td>\n",
       "      <td>Papel higiénico</td>\n",
       "      <td>Productos para el cuidado íntimo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-247</td>\n",
       "      <td>Alíenexpress: Adornos Navideños Creativos... m...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>palio007</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/adorno...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monab</td>\n",
       "      <td>Están divertidos!!!      .   Y hay muchos espa...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185</td>\n",
       "      <td>Soriana: Avena Gerber 270gr</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Soriana</td>\n",
       "      <td>Depredador</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/avena-...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>412</td>\n",
       "      <td>Amazon Control alámbrico blanco Y Negro para X...</td>\n",
       "      <td>605.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>ELPROBADORPRO</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/contro...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tlatuani</td>\n",
       "      <td>Tomado de las reseñas de Amazon Me tomo la mol...</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47</td>\n",
       "      <td>Chedraui: 25% de descuento Bebidas Seltzer Hel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Chedraui</td>\n",
       "      <td>vlma</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/chedra...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>535</td>\n",
       "      <td>WALMART/LAVADORA MABE 21 KG ÚLTIMA LIQUIDACIÓN...</td>\n",
       "      <td>5270.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>CHENTE.</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/walmar...</td>\n",
       "      <td>...</td>\n",
       "      <td>Lavadoras</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oscar_HurTa</td>\n",
       "      <td>Con eso te compras una lavadora gamer jajajaja...</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>Amazon: ADATA Classic Series C008 32 GB USB 2....</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Brayan_SanchezLopez</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/adata-...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>Chedraui: 3 x 2 en Vitamina C Supramed 10 sobr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Chedraui</td>\n",
       "      <td>vlma</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.promodescuentos.com/ofertas/chedra...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Degrees                                            Product  Final_Price  \\\n",
       "0     -139   Amazon Outriders Day One Edition - PlayStation 5        469.0   \n",
       "1      305            Sams Club Homero yogurth griego chobani         40.0   \n",
       "2       91                      Solana: Papel Higiénico Elite         17.0   \n",
       "3     -247  Alíenexpress: Adornos Navideños Creativos... m...         33.0   \n",
       "4      185                        Soriana: Avena Gerber 270gr          5.0   \n",
       "5      412  Amazon Control alámbrico blanco Y Negro para X...        605.0   \n",
       "6       47  Chedraui: 25% de descuento Bebidas Seltzer Hel...          NaN   \n",
       "7      535  WALMART/LAVADORA MABE 21 KG ÚLTIMA LIQUIDACIÓN...       5270.0   \n",
       "8       43  Amazon: ADATA Classic Series C008 32 GB USB 2....         68.0   \n",
       "9       30  Chedraui: 3 x 2 en Vitamina C Supramed 10 sobr...          NaN   \n",
       "\n",
       "   Original_Price  Free_Shipping   Merchant             Username       Date  \\\n",
       "0             NaN           True     Amazon       RobertValencia 2021-10-24   \n",
       "1           150.0          False  Sam'sClub       andres18torres 2021-10-24   \n",
       "2            34.0          False    Soriana           Depredador 2021-10-24   \n",
       "3             NaN          False        NaN             palio007 2021-10-24   \n",
       "4            45.0          False    Soriana           Depredador 2021-10-24   \n",
       "5          1099.0           True     Amazon        ELPROBADORPRO 2021-10-24   \n",
       "6             NaN          False   Chedraui                 vlma 2021-10-24   \n",
       "7             NaN          False    Walmart              CHENTE. 2021-10-24   \n",
       "8             NaN           True     Amazon  Brayan_SanchezLopez 2021-10-24   \n",
       "9             NaN          False   Chedraui                 vlma 2021-10-24   \n",
       "\n",
       "          Origin                                                URL  ...  \\\n",
       "0            NaN  https://www.promodescuentos.com/ofertas/outrid...  ...   \n",
       "1          Local  https://www.promodescuentos.com/ofertas/sams-c...  ...   \n",
       "2            NaN  https://www.promodescuentos.com/ofertas/papel-...  ...   \n",
       "3            NaN  https://www.promodescuentos.com/ofertas/adorno...  ...   \n",
       "4            NaN  https://www.promodescuentos.com/ofertas/avena-...  ...   \n",
       "5  United States  https://www.promodescuentos.com/ofertas/contro...  ...   \n",
       "6            NaN  https://www.promodescuentos.com/ofertas/chedra...  ...   \n",
       "7            NaN  https://www.promodescuentos.com/ofertas/walmar...  ...   \n",
       "8         Mexico  https://www.promodescuentos.com/ofertas/adata-...  ...   \n",
       "9            NaN  https://www.promodescuentos.com/ofertas/chedra...  ...   \n",
       "\n",
       "        Category_3                        Category_4 Category_5 Category_6  \\\n",
       "0    Juegos de PS5                               NaN        NaN        NaN   \n",
       "1              NaN                               NaN        NaN        NaN   \n",
       "2  Papel higiénico  Productos para el cuidado íntimo        NaN        NaN   \n",
       "3              NaN                               NaN        NaN        NaN   \n",
       "4              NaN                               NaN        NaN        NaN   \n",
       "5              NaN                               NaN        NaN        NaN   \n",
       "6              NaN                               NaN        NaN        NaN   \n",
       "7        Lavadoras                               NaN        NaN        NaN   \n",
       "8              NaN                               NaN        NaN        NaN   \n",
       "9              NaN                               NaN        NaN        NaN   \n",
       "\n",
       "   Category_7  Category_8  Category_9  top_comment_user  \\\n",
       "0         NaN         NaN         NaN               NaN   \n",
       "1         NaN         NaN         NaN               NaN   \n",
       "2         NaN         NaN         NaN               NaN   \n",
       "3         NaN         NaN         NaN             Monab   \n",
       "4         NaN         NaN         NaN               NaN   \n",
       "5         NaN         NaN         NaN          tlatuani   \n",
       "6         NaN         NaN         NaN               NaN   \n",
       "7         NaN         NaN         NaN       Oscar_HurTa   \n",
       "8         NaN         NaN         NaN               NaN   \n",
       "9         NaN         NaN         NaN               NaN   \n",
       "\n",
       "                                         top_comment thumbs_up  \n",
       "0                                                NaN       NaN  \n",
       "1                                                NaN       NaN  \n",
       "2                                                NaN       NaN  \n",
       "3  Están divertidos!!!      .   Y hay muchos espa...       9.0  \n",
       "4                                                NaN       NaN  \n",
       "5  Tomado de las reseñas de Amazon Me tomo la mol...      81.0  \n",
       "6                                                NaN       NaN  \n",
       "7  Con eso te compras una lavadora gamer jajajaja...      16.0  \n",
       "8                                                NaN       NaN  \n",
       "9                                                NaN       NaN  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuevas_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to xlsx format to handle encoding\n",
    "# df.to_excel('promodescuentos.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e68d761b7bab1423629162a61be742b8605d4ea6e68ee035843ec55c745be20d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
